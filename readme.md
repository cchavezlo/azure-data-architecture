# ğŸ—ï¸ Arquitectura de Datos en Azure con Databricks + Delta Lake

Este proyecto personal simula un entorno de procesamiento de datos bancarios, implementando un pipeline completo de ingesta, validaciÃ³n, transformaciÃ³n y calidad de datos en la nube con tecnologÃ­as modernas como Azure, Databricks, Spark y Delta Lake.

## ğŸ¯ Objetivos

- Ingestar archivos CSV, Parquet y JSON desde Azure Blob Storage.
- Transformar y limpiar datos con PySpark en Databricks.
- Aplicar reglas de calidad de datos por capas: raw, staging y master.
- Almacenar resultados en Delta Lake para auditorÃ­a y versionado.
- Orquestar el flujo de procesamiento con Azure Data Factory.

## ğŸ§± Arquitectura

![Arquitectura](images/diagrama_arquitectura.png)

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- Azure Blob Storage
- Azure Databricks
- Apache Spark (PySpark)
- Delta Lake
- Azure Data Factory
- Python
- SQL

## ğŸ§ª Reglas de Calidad de Datos

- No nulos en campos obligatorios (`nombre`, `email`)
- Edad entre 0 y 100
- EliminaciÃ³n de duplicados por `id`
- ValidaciÃ³n de correos electrÃ³nicos

Ver [docs/reglas_calidad.md](docs/reglas_calidad.md)

## ğŸš€ CÃ³mo Ejecutar

1. Cargar archivos de muestra en `/data/`
2. Ejecutar los notebooks en el orden:
   - `01_ingesta_raw.ipynb`
   - `02_limpieza_staging.ipynb`
   - `03_validacion_master.ipynb`
3. Validar salidas en Delta Lake y vistas SQL.
4. Orquestar con ADF o scripts adicionales.

## ğŸ“¸ Capturas de Evidencia

- Azure Data Factory:
  ![ADF](images/adf_pipeline.png)

## ğŸ“‚ OrganizaciÃ³n del Repositorio

| Carpeta | DescripciÃ³n |
|---------|-------------|
| `/notebooks` | Notebooks con lÃ³gica ETL |
| `/data` | Archivos simulados para pruebas |
| `/images` | Diagramas y capturas del proyecto |
| `/docs` | Reglas, bitÃ¡cora, documentaciÃ³n adicional |

## ğŸ§  Autor

**Carolina Boniee ChÃ¡vez LÃ³pez**  
[LinkedIn](https://linkedin.com/in/cchavezlo) | [GitHub](https://github.com/cchavezlo)

---

